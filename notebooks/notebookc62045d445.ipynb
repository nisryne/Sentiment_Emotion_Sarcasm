{"cells":[{"cell_type":"markdown","metadata":{"editable":false},"source":["<a id=\"1\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> Twitter (Text classification) </b></div>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<center>\n","<img src=\"https://ai-marketplace.orange-business.com/r/img/Twitter_Sentiment_Analysis.png\" width=800 height=500 />\n","</center>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"border-radius:10px;border:black solid;padding: 15px;background-color:white;font-size:110%;text-align:left\">\n","<div style=\"font-family:Georgia;background-color:'#DEB887'; padding:30px; font-size:17px\">\n","\n","   \n"," \n","<h3 align=\"left\"><font color=purple>üìù Project Description:</font></h3><br> \n","\n","Dataset : The dataset is a combination of  74,681 views from different people in Twitter media which includes 4 columns<br> \n","\n","1-Index : which indicates a number for each sample (not important)<br>\n","2-Borderlands : which indicates a number of areas  for each sample <br>\n","3-Mode : The feeling of that person about the text which has shared<br>\n","4-Text : The texture that view that person has shared.<br>\n","\n","Note:In the dataset , columns have wrong names. The names I mentioned above were right <br>\n","\n","Goals: The goal of this notebook is : <br>\n","1-Importing necessary libraries<br>\n","2-Quick scan of dataset<br>\n","3-EDA(Explatory Data Analysis)<br>\n","4-Data cleaning for text<br>\n","5-Introducing a number of methods in NLP:<br>\n","CounterVectorizer <br>\n","TF-IDF <br>\n","N-Grams <br>\n","POS(Part Of Speech)<br>\n","NER(Named Entity Recognition)<br>\n","Stopwords <br>\n","Lemmas and Stems<br>\n","Tokenization<br>\n","Showing common used words and punctuations <br>\n","Wordcloud illustration <br>\n","\n","6-Preparing dataset  by Pytorch and Keras libraries <br>\n","7-Using Bi-directional LSTM to classify the texts<br>\n","\n","Where we can use of text classification ?Why we use ?<br>\n","Actually, there are varous places which we are able to use of text classification ,but, most importantly<br>\n","in social apps such as Twitter , Instegram ...  and different websites etc ... to avoid sharing inappropriate reviws , discrimination and ... .\n","\n","How we can make better acuracy ?<br>\n","There are vatious ways but the best way increasing more data instead 75K \n","\n","<br> </div>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<a id=\"1\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> Importing Libraries</b></div>"]},{"cell_type":"code","execution_count":9,"metadata":{"editable":false,"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Nisryne\\OneDrive\\Bureau\\nlp\\Emotion-Detection-in-Text-main\\Emotion-Detection-in-Text-main\\notebooks\\notebookc62045d445.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nisryne/OneDrive/Bureau/nlp/Emotion-Detection-in-Text-main/Emotion-Detection-in-Text-main/notebooks/notebookc62045d445.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nisryne/OneDrive/Bureau/nlp/Emotion-Detection-in-Text-main/Emotion-Detection-in-Text-main/notebooks/notebookc62045d445.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nisryne/OneDrive/Bureau/nlp/Emotion-Detection-in-Text-main/Emotion-Detection-in-Text-main/notebooks/notebookc62045d445.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m one_hot\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nisryne/OneDrive/Bureau/nlp/Emotion-Detection-in-Text-main/Emotion-Detection-in-Text-main/notebooks/notebookc62045d445.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nisryne/OneDrive/Bureau/nlp/Emotion-Detection-in-Text-main/Emotion-Detection-in-Text-main/notebooks/notebookc62045d445.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import spacy \n","import warnings\n","import re\n","import string\n","import random\n","\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import RegexpTokenizer , TweetTokenizer\n","from nltk.stem import WordNetLemmatizer ,PorterStemmer\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","from collections import Counter\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import tensorflow as tf\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pandasNote: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script f2py.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","\n","[notice] A new release of pip available: 22.3 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Downloading pandas-2.1.3-cp311-cp311-win_amd64.whl (10.6 MB)\n","     -------------------------------------- 10.6/10.6 MB 110.3 kB/s eta 0:00:00\n","Collecting numpy<2,>=1.23.2\n","  Downloading numpy-1.26.2-cp311-cp311-win_amd64.whl (15.8 MB)\n","     -------------------------------------- 15.8/15.8 MB 305.9 kB/s eta 0:00:00\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n","Collecting pytz>=2020.1\n","  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n","     ------------------------------------ 502.5/502.5 kB 205.9 kB/s eta 0:00:00\n","Collecting tzdata>=2022.1\n","  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n","     ------------------------------------- 341.8/341.8 kB 69.1 kB/s eta 0:00:00\n","Requirement already satisfied: six>=1.5 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Installing collected packages: pytz, tzdata, numpy, pandas\n","Successfully installed numpy-1.26.2 pandas-2.1.3 pytz-2023.3.post1 tzdata-2023.3\n"]}],"source":["pip install pandas"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.2)\n","Collecting matplotlib\n","  Downloading matplotlib-3.8.2-cp311-cp311-win_amd64.whl (7.6 MB)\n","     ---------------------------------------- 7.6/7.6 MB 347.1 kB/s eta 0:00:00\n","Collecting seaborn\n","  Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n","     -------------------------------------- 294.6/294.6 kB 1.8 MB/s eta 0:00:00\n","Collecting spacy\n","  Downloading spacy-3.7.2-cp311-cp311-win_amd64.whl (12.1 MB)\n","     -------------------------------------- 12.1/12.1 MB 272.4 kB/s eta 0:00:00\n","Collecting contourpy>=1.0.1\n","  Downloading contourpy-1.2.0-cp311-cp311-win_amd64.whl (187 kB)\n","     ------------------------------------ 187.6/187.6 kB 257.7 kB/s eta 0:00:00\n","Collecting cycler>=0.10\n","  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Collecting fonttools>=4.22.0\n","  Downloading fonttools-4.44.3-cp311-cp311-win_amd64.whl (2.1 MB)\n","     ---------------------------------------- 2.1/2.1 MB 744.2 kB/s eta 0:00:00\n","Collecting kiwisolver>=1.3.1\n","  Downloading kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n","     ---------------------------------------- 56.1/56.1 kB ? eta 0:00:00\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (21.3)\n","Collecting pillow>=8\n","  Downloading Pillow-10.1.0-cp311-cp311-win_amd64.whl (2.6 MB)\n","     ---------------------------------------- 2.6/2.6 MB 316.8 kB/s eta 0:00:00\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pandas>=1.2 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn) (2.1.3)\n","Collecting spacy-legacy<3.1.0,>=3.0.11\n","  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n","Collecting murmurhash<1.1.0,>=0.28.0\n","  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n","Collecting cymem<2.1.0,>=2.0.2\n","  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n","Collecting preshed<3.1.0,>=3.0.2\n","  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n","     ------------------------------------ 122.3/122.3 kB 448.3 kB/s eta 0:00:00\n","Collecting thinc<8.3.0,>=8.1.8\n","  Downloading thinc-8.2.1-cp311-cp311-win_amd64.whl (1.5 MB)\n","     ---------------------------------------- 1.5/1.5 MB 1.0 MB/s eta 0:00:00\n","Collecting wasabi<1.2.0,>=0.9.1\n","  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n","Collecting srsly<3.0.0,>=2.4.3\n","  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n","     -------------------------------------- 479.7/479.7 kB 1.1 MB/s eta 0:00:00\n","Collecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n","Collecting weasel<0.4.0,>=0.1.0\n","  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n","     ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n","Collecting typer<0.10.0,>=0.3.0\n","  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n","     ---------------------------------------- 45.9/45.9 kB ? eta 0:00:00\n","Collecting smart-open<7.0.0,>=5.2.1\n","  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n","     -------------------------------------- 57.0/57.0 kB 213.7 kB/s eta 0:00:00\n","Collecting tqdm<5.0.0,>=4.38.0\n","  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n","     -------------------------------------- 78.3/78.3 kB 723.6 kB/s eta 0:00:00\n","Collecting requests<3.0.0,>=2.13.0\n","  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n","     ---------------------------------------- 62.6/62.6 kB 1.7 MB/s eta 0:00:00\n","Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n","  Downloading pydantic-2.5.1-py3-none-any.whl (381 kB)\n","     ------------------------------------ 381.6/381.6 kB 383.2 kB/s eta 0:00:00\n","Requirement already satisfied: jinja2 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","     ------------------------------------ 181.6/181.6 kB 577.1 kB/s eta 0:00:00\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n","Collecting annotated-types>=0.4.0\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.14.3\n","  Downloading pydantic_core-2.14.3-cp311-none-win_amd64.whl (1.9 MB)\n","     ---------------------------------------- 1.9/1.9 MB 174.8 kB/s eta 0:00:00\n","Collecting typing-extensions>=4.6.1\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: six>=1.5 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting charset-normalizer<4,>=2\n","  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n","     -------------------------------------- 99.9/99.9 kB 133.4 kB/s eta 0:00:00\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Collecting urllib3<3,>=1.21.1\n","  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n","     ------------------------------------ 104.6/104.6 kB 116.0 kB/s eta 0:00:00\n","Collecting certifi>=2017.4.17\n","  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n","     ------------------------------------ 162.5/162.5 kB 149.9 kB/s eta 0:00:00\n","Collecting blis<0.8.0,>=0.7.8\n","  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n","     ---------------------------------------- 6.6/6.6 MB 1.1 MB/s eta 0:00:00\n","Collecting confection<1.0.0,>=0.0.1\n","  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n","Requirement already satisfied: colorama in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n","Collecting click<9.0.0,>=7.1.1\n","  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n","     ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n","Collecting cloudpathlib<0.17.0,>=0.7.0\n","  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n","     ---------------------------------------- 45.0/45.0 kB 1.1 MB/s eta 0:00:00\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->spacy) (2.1.1)\n","Installing collected packages: cymem, wasabi, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, smart-open, pillow, murmurhash, langcodes, kiwisolver, fonttools, cycler, contourpy, cloudpathlib, click, charset-normalizer, certifi, catalogue, blis, annotated-types, typer, srsly, requests, pydantic-core, preshed, matplotlib, seaborn, pydantic, confection, weasel, thinc, spacy\n","Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 cloudpathlib-0.16.0 confection-0.1.3 contourpy-1.2.0 cycler-0.12.1 cymem-2.0.8 fonttools-4.44.3 kiwisolver-1.4.5 langcodes-3.3.0 matplotlib-3.8.2 murmurhash-1.0.10 pillow-10.1.0 preshed-3.0.9 pydantic-2.5.1 pydantic-core-2.14.3 requests-2.31.0 seaborn-0.13.0 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 tqdm-4.66.1 typer-0.9.0 typing-extensions-4.8.0 urllib3-2.1.0 wasabi-1.1.2 weasel-0.3.4\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script tqdm.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script normalizer.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script weasel.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script spacy.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","\n","[notice] A new release of pip available: 22.3 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install numpy matplotlib seaborn spacy"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nltk\n","  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","     ---------------------------------------- 1.5/1.5 MB 251.4 kB/s eta 0:00:00\n","Requirement already satisfied: click in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n","Collecting joblib\n","  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n","     ------------------------------------ 302.2/302.2 kB 203.0 kB/s eta 0:00:00\n","Collecting regex>=2021.8.3\n","  Downloading regex-2023.10.3-cp311-cp311-win_amd64.whl (269 kB)\n","     ------------------------------------ 269.6/269.6 kB 720.3 kB/s eta 0:00:00\n","Requirement already satisfied: tqdm in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n","Installing collected packages: regex, joblib, nltk\n","Successfully installed joblib-1.3.2 nltk-3.8.1 regex-2023.10.3\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script nltk.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","\n","[notice] A new release of pip available: 22.3 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install nltk\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n","\n","  Downloading tensorflow-2.15.0-cp311-cp311-win_amd64.whl (2.1 kB)\n","Collecting tensorflow-intel==2.15.0\n","  Downloading tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl (300.9 MB)\n","     ------------------------------------- 300.9/300.9 MB 52.3 kB/s eta 0:00:00\n","Collecting absl-py>=1.0.0\n","  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n","     ------------------------------------- 130.2/130.2 kB 91.4 kB/s eta 0:00:00\n","Collecting astunparse>=1.6.0\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Collecting flatbuffers>=23.5.26\n","  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n","Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n","  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n","Collecting google-pasta>=0.1.1\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","     -------------------------------------- 57.5/57.5 kB 144.0 kB/s eta 0:00:00\n","Collecting h5py>=2.9.0\n","  Downloading h5py-3.10.0-cp311-cp311-win_amd64.whl (2.7 MB)\n","     ---------------------------------------- 2.7/2.7 MB 35.0 kB/s eta 0:00:00\n","Collecting libclang>=13.0.0\n","  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n","     -------------------------------------- 24.4/24.4 MB 278.6 kB/s eta 0:00:00\n","Collecting ml-dtypes~=0.2.0\n","  Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n","     ------------------------------------ 938.7/938.7 kB 182.3 kB/s eta 0:00:00\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n","Collecting opt-einsum>=2.3.2\n","  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","     -------------------------------------- 65.5/65.5 kB 196.6 kB/s eta 0:00:00\n","Requirement already satisfied: packaging in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (21.3)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n","  Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl (413 kB)\n","     ------------------------------------ 413.4/413.4 kB 330.7 kB/s eta 0:00:00\n","Requirement already satisfied: setuptools in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.5.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n","Collecting termcolor>=1.1.0\n","  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.8.0)\n","Collecting wrapt<1.15,>=1.11.0\n","  Downloading wrapt-1.14.1-cp311-cp311-win_amd64.whl (35 kB)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n","     ---------------------------------------- 1.5/1.5 MB 325.8 kB/s eta 0:00:00\n","Collecting grpcio<2.0,>=1.24.3\n","  Downloading grpcio-1.59.3-cp311-cp311-win_amd64.whl (3.7 MB)\n","     ---------------------------------------- 3.7/3.7 MB 1.0 MB/s eta 0:00:00\n","Collecting tensorboard<2.16,>=2.15\n","  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n","     ---------------------------------------- 5.5/5.5 MB 1.0 MB/s eta 0:00:00\n","Collecting tensorflow-estimator<2.16,>=2.15.0\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","     -------------------------------------- 442.0/442.0 kB 2.0 MB/s eta 0:00:00\n","Collecting keras<2.16,>=2.15.0\n","  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","     ---------------------------------------- 1.7/1.7 MB 1.2 MB/s eta 0:00:00\n","Collecting wheel<1.0,>=0.23.0\n","  Downloading wheel-0.41.3-py3-none-any.whl (65 kB)\n","     ---------------------------------------- 65.8/65.8 kB 1.7 MB/s eta 0:00:00\n","Collecting google-auth<3,>=1.6.3\n","  Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n","     -------------------------------------- 183.3/183.3 kB 2.2 MB/s eta 0:00:00\n","Collecting google-auth-oauthlib<2,>=0.5\n","  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n","Collecting markdown>=2.6.8\n","  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n","     ------------------------------------ 102.2/102.2 kB 589.2 kB/s eta 0:00:00\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n","  Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl (422 kB)\n","     ------------------------------------ 422.5/422.5 kB 732.2 kB/s eta 0:00:00\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0\n","  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n","Collecting werkzeug>=1.0.1\n","  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n","     -------------------------------------- 226.7/226.7 kB 1.2 MB/s eta 0:00:00\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from packaging->tensorflow-intel==2.15.0->tensorflow) (3.0.9)\n","Collecting cachetools<6.0,>=2.0.0\n","  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n","Collecting pyasn1-modules>=0.2.1\n","  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n","     -------------------------------------- 181.3/181.3 kB 1.2 MB/s eta 0:00:00\n","Collecting rsa<5,>=3.1.4\n","  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n","Collecting requests-oauthlib>=0.7.0\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n","Collecting pyasn1<0.6.0,>=0.4.6\n","  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n","     ---------------------------------------- 83.9/83.9 kB 1.6 MB/s eta 0:00:00\n","Collecting oauthlib>=3.0.0\n","  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","     ------------------------------------ 151.7/151.7 kB 696.8 kB/s eta 0:00:00\n","Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n","Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.4 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 werkzeug-3.0.1 wheel-0.41.3 wrapt-1.14.1\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script wheel.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script markdown_py.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script google-oauthlib-tool.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The script tensorboard.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","\n","[notice] A new release of pip available: 22.3 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install --upgrade pip"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wordcloud\n","  Downloading wordcloud-1.9.2-cp311-cp311-win_amd64.whl (151 kB)\n","     ------------------------------------ 151.4/151.4 kB 600.8 kB/s eta 0:00:00\n","Requirement already satisfied: numpy>=1.6.1 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (1.26.2)\n","Requirement already satisfied: pillow in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (10.1.0)\n","Requirement already satisfied: matplotlib in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wordcloud) (3.8.2)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (4.44.3)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nisryne\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (21.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (2.8.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\nisryne\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n","Installing collected packages: wordcloud\n","Successfully installed wordcloud-1.9.2\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["  WARNING: The script wordcloud_cli.exe is installed in 'c:\\Users\\Nisryne\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","\n","[notice] A new release of pip available: 22.3 -> 23.3.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["pip install wordcloud\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* First we load the dataset and show 7 random samples of it<br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:41.055304Z","iopub.status.busy":"2023-11-03T10:41:41.05419Z","iopub.status.idle":"2023-11-03T10:41:41.418374Z","shell.execute_reply":"2023-11-03T10:41:41.417521Z","shell.execute_reply.started":"2023-11-03T10:41:41.055272Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('C:\\Users\\Nisryne\\OneDrive\\Bureau\\nlp\\Emotion-Detection-in-Text-main\\Emotion-Detection-in-Text-main\\data\\twitter_training.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:43.274515Z","iopub.status.busy":"2023-11-03T10:41:43.273649Z","iopub.status.idle":"2023-11-03T10:41:43.30511Z","shell.execute_reply":"2023-11-03T10:41:43.304153Z","shell.execute_reply.started":"2023-11-03T10:41:43.274474Z"},"trusted":true},"outputs":[],"source":["df.sample(7)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* Then , a qucik view of shape , names of columns and types of dataset : <br>\n","shape is : 74681 rows and 4 columns <br>\n","Name of columns are not good and must be changed<br>\n","types of columns are in the right situation.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:45.87866Z","iopub.status.busy":"2023-11-03T10:41:45.877739Z","iopub.status.idle":"2023-11-03T10:41:45.884804Z","shell.execute_reply":"2023-11-03T10:41:45.882941Z","shell.execute_reply.started":"2023-11-03T10:41:45.878621Z"},"trusted":true},"outputs":[],"source":["print(f'The shape of the dataset is : {df.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:48.220674Z","iopub.status.busy":"2023-11-03T10:41:48.219781Z","iopub.status.idle":"2023-11-03T10:41:48.225418Z","shell.execute_reply":"2023-11-03T10:41:48.224475Z","shell.execute_reply.started":"2023-11-03T10:41:48.22064Z"},"trusted":true},"outputs":[],"source":["print(f'The columns are :{df.columns}')"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:50.171363Z","iopub.status.busy":"2023-11-03T10:41:50.170534Z","iopub.status.idle":"2023-11-03T10:41:50.176567Z","shell.execute_reply":"2023-11-03T10:41:50.175703Z","shell.execute_reply.started":"2023-11-03T10:41:50.17133Z"},"trusted":true},"outputs":[],"source":["print(f'The dtypes of the dataset : \\n\\n{df.dtypes}')"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* In a quick review we can observe basic information <br>\n","* But we are going to create a function to show of more details such as<br>\n","Volume of null values and its percentage , Volume of duplicated and percentage of duplicated values\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:52.346863Z","iopub.status.busy":"2023-11-03T10:41:52.346016Z","iopub.status.idle":"2023-11-03T10:41:52.456573Z","shell.execute_reply":"2023-11-03T10:41:52.455602Z","shell.execute_reply.started":"2023-11-03T10:41:52.346833Z"},"trusted":true},"outputs":[],"source":["df.describe(include='all')"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<a id=\"1\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> EDA</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:55.42718Z","iopub.status.busy":"2023-11-03T10:41:55.426505Z","iopub.status.idle":"2023-11-03T10:41:55.432678Z","shell.execute_reply":"2023-11-03T10:41:55.431768Z","shell.execute_reply.started":"2023-11-03T10:41:55.427149Z"},"trusted":true},"outputs":[],"source":["def show_details(dataset):\n","    missed_values = dataset.isnull().sum()\n","    missed_values_percent = (dataset.isnull().sum()) / len(dataset)\n","    duplicated_values = dataset.duplicated().sum()\n","    duplicated_values_percent = (dataset.duplicated().sum()) / len(dataset)\n","    info_frame = pd.DataFrame({'Missed_Values' : missed_values , \n","                              'Missed_Values %' :missed_values_percent,\n","                              'Duplicated values' :duplicated_values,\n","                              'Duplicated values %':duplicated_values_percent})\n","    return info_frame.T"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* We can see that very small number of data are missed and duplicated which can easily be dropped.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:41:58.089197Z","iopub.status.busy":"2023-11-03T10:41:58.088837Z","iopub.status.idle":"2023-11-03T10:41:58.216925Z","shell.execute_reply":"2023-11-03T10:41:58.216018Z","shell.execute_reply.started":"2023-11-03T10:41:58.089169Z"},"trusted":true},"outputs":[],"source":["show_details(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:00.297178Z","iopub.status.busy":"2023-11-03T10:42:00.296808Z","iopub.status.idle":"2023-11-03T10:42:00.508758Z","shell.execute_reply":"2023-11-03T10:42:00.507794Z","shell.execute_reply.started":"2023-11-03T10:42:00.29715Z"},"trusted":true},"outputs":[],"source":["df.drop_duplicates(inplace=True)\n","df.dropna(inplace=True)\n","show_details(df)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* Here we change the name of columns to  appropriate names:<br>\n","'2401' : 'Index' , 'Borderlands': 'Land' , 'Positive' : 'Mode' , \"im getting on borderlands and i will murder you all ,\": 'Text'\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:03.40474Z","iopub.status.busy":"2023-11-03T10:42:03.403886Z","iopub.status.idle":"2023-11-03T10:42:03.409603Z","shell.execute_reply":"2023-11-03T10:42:03.408647Z","shell.execute_reply.started":"2023-11-03T10:42:03.404703Z"},"trusted":true},"outputs":[],"source":["df.rename(columns={'2401' : 'Index' , 'Borderlands': 'Land' , 'Positive' : 'Mode' \n","                   , \"im getting on borderlands and i will murder you all ,\": 'Text'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:05.845215Z","iopub.status.busy":"2023-11-03T10:42:05.84438Z","iopub.status.idle":"2023-11-03T10:42:05.851128Z","shell.execute_reply":"2023-11-03T10:42:05.850306Z","shell.execute_reply.started":"2023-11-03T10:42:05.845181Z"},"trusted":true},"outputs":[],"source":["df.columns"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* Here , we check Lands column , the number of unique values which is 32  and its names\n","* And  show the number of each values by dataframe and a Barchart\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:08.076953Z","iopub.status.busy":"2023-11-03T10:42:08.076359Z","iopub.status.idle":"2023-11-03T10:42:08.094743Z","shell.execute_reply":"2023-11-03T10:42:08.093748Z","shell.execute_reply.started":"2023-11-03T10:42:08.076911Z"},"trusted":true},"outputs":[],"source":["print(f'The number of unique lands : {len(df.Land.unique())}')\n","print('**' * 40)\n","df.Land.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:10.312016Z","iopub.status.busy":"2023-11-03T10:42:10.311295Z","iopub.status.idle":"2023-11-03T10:42:10.329825Z","shell.execute_reply":"2023-11-03T10:42:10.328894Z","shell.execute_reply.started":"2023-11-03T10:42:10.311988Z"},"trusted":true},"outputs":[],"source":["lands =df.Land.value_counts()\n","lands.to_frame()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* We can see that distribution is quite the same for each area .<br>\n","* between the range 2150 to 2328 changeably \n","* Now we just show 10 of the hihest ones\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:13.344594Z","iopub.status.busy":"2023-11-03T10:42:13.343867Z","iopub.status.idle":"2023-11-03T10:42:13.72941Z","shell.execute_reply":"2023-11-03T10:42:13.728492Z","shell.execute_reply.started":"2023-11-03T10:42:13.344562Z"},"trusted":true},"outputs":[],"source":["sns.set_style('darkgrid')\n","plt.figure(figsize=(10,6))\n","bar = sns.barplot(x=lands.values[:10] ,y=lands.index[:10] , palette='rocket')\n","bar.bar_label(bar.containers[0])\n","plt.title('Top 10 Lands')\n","plt.xlabel('Count')\n","plt.ylabel('Land')\n","plt.xlim(0 , 2500)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* column Mode , which consists of 4 modes as  : Positive ,Neutral ,Negative and Irrelevant<br>\n","Negative with highest common reviws : 21698 <br>\n","Positive the second : 19712<br>\n","Neutral the third : 17708<br>\n","Irrelevant : 12537<br>\n","* Lets depict the values by a pie chart \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:16.910337Z","iopub.status.busy":"2023-11-03T10:42:16.909737Z","iopub.status.idle":"2023-11-03T10:42:16.926529Z","shell.execute_reply":"2023-11-03T10:42:16.925566Z","shell.execute_reply.started":"2023-11-03T10:42:16.910304Z"},"trusted":true},"outputs":[],"source":["print(f'The unique values of Mode : {len(df.Mode.unique())}')\n","print('**' * 20)\n","print(df.Mode.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:19.297622Z","iopub.status.busy":"2023-11-03T10:42:19.296625Z","iopub.status.idle":"2023-11-03T10:42:19.316269Z","shell.execute_reply":"2023-11-03T10:42:19.315286Z","shell.execute_reply.started":"2023-11-03T10:42:19.297581Z"},"trusted":true},"outputs":[],"source":["mode = df.Mode.value_counts()\n","mode.to_frame().T"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","\n","Negative : 30.3 % <br>\n","Positive : 27.5%<br>\n","Neutral : 24.7%<br>\n","Irrelevant : 17.5%<br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:22.513393Z","iopub.status.busy":"2023-11-03T10:42:22.512936Z","iopub.status.idle":"2023-11-03T10:42:22.724605Z","shell.execute_reply":"2023-11-03T10:42:22.72334Z","shell.execute_reply.started":"2023-11-03T10:42:22.513356Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(6,6))\n","plt.pie(x = mode.values , labels=mode.keys() ,autopct=\"%1.1f%%\" , \n","textprops={\"fontsize\":10,\"fontweight\":\"black\"},colors=sns.color_palette(\"rocket\"))\n","plt.title('Mode Distribution') \n","plt.show()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* we can also take a look at each land with its mode and scan them : <br>\n","\n","Irrelevant : Battlefield  has highest with 907 , TomClancysGhostRecon with 92 Lowest<br>\n","\n","Negative :   MaddenNFL  has highest with 1665 , RedDeadRedemption(RDR) with 290 Lowest<br>\n","\n","Neutral :    Amazon  has highest with 1197 , AssassinsCreed with 153 Lowest<br>\n","\n","Positive :   AssassinsCreed  has highest with 1382 , Facebook\t with 154 Lowest<br>\n","\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:27.665998Z","iopub.status.busy":"2023-11-03T10:42:27.665531Z","iopub.status.idle":"2023-11-03T10:42:27.781925Z","shell.execute_reply":"2023-11-03T10:42:27.781113Z","shell.execute_reply.started":"2023-11-03T10:42:27.665961Z"},"trusted":true},"outputs":[],"source":["pd.crosstab(df.Mode , df.Land).T.style.background_gradient( subset=['Negative'],cmap='Reds')\\\n",".background_gradient(subset=['Positive'] , cmap='Greens')\\\n",".background_gradient(subset=['Irrelevant'] , cmap='BuGn')\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* Here with re library we can replace a number of grammatically problems , verbal expressions , ... <br>\n","* However we can deal with various emojis which are used from people to show off their emotion about the post(replace or delete them) <br>\n","* Needless to say , emojis can play important role in determining the class in the views<br>\n","* in the last part of text_clear function we have replaced more than 1 punctuations(those we have allowed to be) to just 1<br>\n","* Finally , all texts become lower mode and instead more than 1 space be just 1 space<br> \n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:31.886921Z","iopub.status.busy":"2023-11-03T10:42:31.886036Z","iopub.status.idle":"2023-11-03T10:42:31.898522Z","shell.execute_reply":"2023-11-03T10:42:31.897502Z","shell.execute_reply.started":"2023-11-03T10:42:31.886889Z"},"trusted":true},"outputs":[],"source":["def clean_emoji(tx):\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols \n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport \n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    \n","    return emoji_pattern.sub(r'', tx)\n","def text_cleaner(tx):\n","    \n","    text = re.sub(r\"won\\'t\", \"would not\", tx)\n","    text = re.sub(r\"im\", \"i am\", tx)\n","    text = re.sub(r\"Im\", \"I am\", tx)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"don\\'t\", \"do not\", text)\n","    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n","    text = re.sub(r\"needn\\'t\", \"need not\", text)\n","    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n","    text = re.sub(r\"haven\\'t\", \"have not\", text)\n","    text = re.sub(r\"weren\\'t\", \"were not\", text)\n","    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n","    text = re.sub(r\"didn\\'t\", \"did not\", text)\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n","    text = re.sub(r'[!]+' , '!' , text)\n","    text = re.sub(r'[?]+' , '?' , text)\n","    text = re.sub(r'[.]+' , '.' , text)\n","    text = re.sub(r'[@]+' , '@' , text)\n","    text = re.sub(r'unk' , ' ' , text)\n","    text = re.sub('\\n', '', text)\n","    text = text.lower()\n","    text = re.sub(r'[ ]+' , ' ' , text)\n","    \n","    return text\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Description**<br>\n","* Now , before replacing all dataset , I want to go through a copule of common methods which are familiar in NLP<br>\n","for that I need to have a random example from dataset which I am using of random.choice to get one sample and clean with the function<br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:35.110122Z","iopub.status.busy":"2023-11-03T10:42:35.109742Z","iopub.status.idle":"2023-11-03T10:42:35.123415Z","shell.execute_reply":"2023-11-03T10:42:35.1225Z","shell.execute_reply.started":"2023-11-03T10:42:35.110077Z"},"trusted":true},"outputs":[],"source":["random.seed(99)\n","test_text =text_cleaner( random.choice(df['Text']))\n","test_text = clean_emoji(test_text)\n","test_text"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["POS => Part Of Speech"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**POS:**<br>\n","\n","POS or Part Of Speech ,  the part of speech indicates how the word functions in meaning as well as grammatically within the sentence.<br>\n","There are 8 parts in English (noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection)<br>\n","Understanding parts of speech is essential for determining the correct definition of a word when using the dictionary.\n","\n","\n","**Description**<br>\n","with using of Spacy library and its amazing features (nlp = spacy.load(\"en_core_web_sm\")) which is used for English language<br>\n","I am going to show you POS in the particular sample <br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:38.783725Z","iopub.status.busy":"2023-11-03T10:42:38.783388Z","iopub.status.idle":"2023-11-03T10:42:38.813283Z","shell.execute_reply":"2023-11-03T10:42:38.812286Z","shell.execute_reply.started":"2023-11-03T10:42:38.783698Z"},"trusted":true},"outputs":[],"source":["doc = nlp(test_text)\n","for token in doc :\n","    print(f'{token} => {token.pos_}')"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["Named Entity Recognition"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**NER:**<br>\n","\n","\n","NER or Named Entity Recognition , identifies, categorizes and extracts(named entities in text) the most important pieces of information from<br> unstructured text without\n","requiring time-consuming human analysis. It's particularly useful for quickly extracting key information from large <br>amounts of data because it\n","automates the extraction process<br>\n","Furthermore , Named entities are specific terms that represent real-world objects, such as people, organizations, locations, and dates.\n","\n","\n","**Description**<br>\n","like the previous exmple and spacy library features Im showing NER on our sample <br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:44.171445Z","iopub.status.busy":"2023-11-03T10:42:44.170735Z","iopub.status.idle":"2023-11-03T10:42:44.188897Z","shell.execute_reply":"2023-11-03T10:42:44.187876Z","shell.execute_reply.started":"2023-11-03T10:42:44.171414Z"},"trusted":true},"outputs":[],"source":["doc = nlp(test_text)\n","for chunk in doc.ents:\n","    print(f'{chunk} => {chunk.label_}')\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Chunking:**<br>\n","Converting a text to a smaller peices for having better undrestanding or the process of grouping similar words together based on <br>the nature of the word.<br>\n","Noun Groups, Verbs, verb groups, etc. <br>\n","\n","NP stands for : Noun Chunks<br>\n","VP : Verp Chunks\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:47.856335Z","iopub.status.busy":"2023-11-03T10:42:47.855978Z","iopub.status.idle":"2023-11-03T10:42:47.872335Z","shell.execute_reply":"2023-11-03T10:42:47.87128Z","shell.execute_reply.started":"2023-11-03T10:42:47.856307Z"},"trusted":true},"outputs":[],"source":["doc = nlp(test_text)\n","for chunk in doc.noun_chunks:\n","    print(f'{chunk} => {chunk.label_}')"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Tokenization :**<br>\n","\n","Tokenization is breaking text into smaller parts for easier machine analysis, helping machines understand human language.<br>\n","These smaller parts known as tokens <br>\n","\n","Note : <br>\n","There are various types of tokenizations such as : RegexpTokenizaton , TweetTokenization and etc...<br>\n","which each one has different method for breaking a text into tokens <br>\n","\n","**Description**<br>\n","Here we are using Regexp tokenization which splits a string into substrings using a regular expression. base on space<br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:50.395505Z","iopub.status.busy":"2023-11-03T10:42:50.395154Z","iopub.status.idle":"2023-11-03T10:42:50.402362Z","shell.execute_reply":"2023-11-03T10:42:50.401507Z","shell.execute_reply.started":"2023-11-03T10:42:50.395477Z"},"trusted":true},"outputs":[],"source":["# Tokenizer = TweetTokenizer()\n","Tokenizer=RegexpTokenizer(r'\\w+')\n","test_text_tokenized = Tokenizer.tokenize(test_text)\n","test_text_tokenized\n","\n","# df['Text']=df['Text'].apply(lambda x : Tokenizer.tokenize(x))"]},{"cell_type":"markdown","metadata":{"editable":false},"source":[" Counter vectorizer"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**CountVectorizer:**<br>\n","\n","Count Vectorizer is used to convert documents, text into vectors of term or token counts, it involves counting the number of occurences of words<br> appears in a document.For example , in our sentence we can see that is has come 2 times and other words have come just once.So,there will be a<br>\n","vector which depicts the number of times which each word comes in the sentence.For better understanding , Im going to show in a heatmap chart.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:53.313636Z","iopub.status.busy":"2023-11-03T10:42:53.312846Z","iopub.status.idle":"2023-11-03T10:42:53.786092Z","shell.execute_reply":"2023-11-03T10:42:53.785051Z","shell.execute_reply.started":"2023-11-03T10:42:53.313605Z"},"trusted":true},"outputs":[],"source":["\n","words  = ['ghost','of','tsushima','now','graphically','is','best','open','world','red','dead','redemption','2','one','second','ahead']\n","counter_vectorizer = CountVectorizer()\n","transform = counter_vectorizer.fit_transform([test_text]).toarray()\n","sns.heatmap(transform, annot=True,xticklabels=words, \n","        cbar=False)\n","transform"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**TF-IDF:**<br>\n","\n","Tf_IDF : Actually , TF-IDF is composed of two parts.Firtsly , Tf which stands for Term Frequancy is how many times a word appears in a document.<br>\n","(counting the number of words and divide it to the number of all words in the sentence)<br>\n","\n","IDF : which stands for Inverse Document Frequancy , is how common a word is found in a corpus or how uncommon a word is found in a corpus.<br>\n","(measure of how important a term is across all documents in the corpus)<br>\n","Result is actually a number between 0 and 1 , It is calculated by taking the logarithm of the total number of documents in the corpus divided<br>\n","by number the of documents in which the term appears<br>\n","\n","However , it can not be a perfect manner to show just a single example to describe TF-IDF , but I will show it for better understanding <br>\n","\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:42:56.961602Z","iopub.status.busy":"2023-11-03T10:42:56.961249Z","iopub.status.idle":"2023-11-03T10:42:57.432426Z","shell.execute_reply":"2023-11-03T10:42:57.431537Z","shell.execute_reply.started":"2023-11-03T10:42:56.961573Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","words  = ['ghost','of','tsushima','now','graphically','is','best','open','world','red','dead','redemption','2','one','second','ahead']\n","TF_IDF = TfidfVectorizer()\n","transform = TF_IDF.fit_transform([test_text]).toarray()\n","sns.heatmap(transform, annot=True,xticklabels=words, \n","        cbar=False)\n","transform"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**N-grams:**<br>\n","\n","In a nutshell , N-gram means a sequence of N words.a collection of n successive items in a text document that may include words, numbers, symbols,<br>  and punctuation.If 2 , it is based on just the word and the next word after that.3 grams , itself and with 2 words after that and so on. <br>\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["N-grams => 3-grams "]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:00.419168Z","iopub.status.busy":"2023-11-03T10:43:00.418714Z","iopub.status.idle":"2023-11-03T10:43:00.429546Z","shell.execute_reply":"2023-11-03T10:43:00.428522Z","shell.execute_reply.started":"2023-11-03T10:43:00.41913Z"},"trusted":true},"outputs":[],"source":["def n_grams(text, n):\n","\n","    return [text[i:i+n] for i in range(len(text)-n+1)]\n","cleaned = test_text_tokenized\n","n_grams(cleaned, 3)\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Stop words:**<br>\n","\n","Stop words are words which are very common in a language. In many projects , they are deleted because they can not affect and they <br>\n","can easily increase the volume of texts without any assistance.<br>\n","\n","Actually , there is a package which contains all stopwords(in NLTK library) in English which are commonly used for NLP projects.<br>\n","First we have to determine our language and download the words for that language.<br>\n","As you can see , there are 179 stopwords in English and I have shown 20 of them .\n","\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:03.595568Z","iopub.status.busy":"2023-11-03T10:43:03.594812Z","iopub.status.idle":"2023-11-03T10:43:03.605563Z","shell.execute_reply":"2023-11-03T10:43:03.604537Z","shell.execute_reply.started":"2023-11-03T10:43:03.595537Z"},"trusted":true},"outputs":[],"source":["stopwords_list = stopwords.words('english')\n","print(f'There are {len(stopwords_list) } stop words')\n","print('**' * 20 , '\\n20 of them are as follows:\\n')\n","for inx , value in enumerate(stopwords_list[:20]):\n","    print(f'{inx+1}:{value}')\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Punctuation count**<br>\n","\n","Now I want to show what punctuations are mostly used in each Mode of views.First, I create a function to read words of each sample for each <br>\n","Mode and then using defaultdict to count each stop words which exists in the whole samples.Using sorted to adjust them with highest one to lowest<br>\n","(for 10 most commons , I have used Bar chart to depict)<br>\n","\n","You can see that , (the , to , and , a) are come the same amount in each Mode with same distribution respectively . \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:07.301226Z","iopub.status.busy":"2023-11-03T10:43:07.300387Z","iopub.status.idle":"2023-11-03T10:43:07.307448Z","shell.execute_reply":"2023-11-03T10:43:07.306541Z","shell.execute_reply.started":"2023-11-03T10:43:07.301186Z"},"trusted":true},"outputs":[],"source":["def make_corpus(kind):\n","    corpus = []\n","    for text in df.loc[df['Mode']==kind]['Text'].str.split():\n","        for word in text:\n","            corpus.append(word)\n","    return corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:09.417731Z","iopub.status.busy":"2023-11-03T10:43:09.417014Z","iopub.status.idle":"2023-11-03T10:43:14.00019Z","shell.execute_reply":"2023-11-03T10:43:13.999164Z","shell.execute_reply.started":"2023-11-03T10:43:09.4177Z"},"trusted":true},"outputs":[],"source":["%%time\n","stop = stopwords.words('english')\n","sentiments = list(df.Mode.unique())\n","\n","\n","\n","for inx , value in enumerate(sentiments):\n","    \n","    corpus = make_corpus(value)\n","    \n","    dic = defaultdict(int)\n","\n","    for word in corpus:\n","        if word in stop:\n","            dic[word] += 1\n","    \n","    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n","\n","    x, y = zip(*top)\n","    plt.title(f'{value} ')\n","    plt.bar(x , y)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","\n","Here , I just do the previous techniques on the dataset to prepare them for the next steps.<br>\n","clear them with text_clearer , and tokenize them . \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:17.266989Z","iopub.status.busy":"2023-11-03T10:43:17.26629Z","iopub.status.idle":"2023-11-03T10:43:22.845625Z","shell.execute_reply":"2023-11-03T10:43:22.844735Z","shell.execute_reply.started":"2023-11-03T10:43:17.266957Z"},"trusted":true},"outputs":[],"source":["# df['Text'] = df['Text'].apply(lambda x : clean_emoji(x))\n","df['Text'] = df['Text'].apply(lambda x : text_cleaner(x))\n","df['Text']= df['Text'].apply(lambda x : Tokenizer.tokenize(x))\n","df['Text'].to_frame()\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["Lemmas and Stems"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Lemmatization :**<br>\n","\n","Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to <br> \n","identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good. <br>\n","\n","**Stemming :**<br>\n","Stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form.The word <br>\n","after stemming is called stem of that word. <br>\n","\n","**Difference :**<br>\n","The most important different between Lemmatization and  stemming  is that , Lemmatization is more accurate and it brings a word to the <br>\n","language root of that word and stemming can be anything which means for computer and is not readable for humans(sometimes not readable)<br>\n","However , stemming is also faster and is a great way for huge corpus.\n","\n","**Sample**:<br>\n","In the example I have shown the lemma of each word \n","\n","**Dataset**<br>\n","for the all dataset , we can afford either , which I have commented lemmatization method , if we want Lemmatization we can uncomment it and use<br>\n","instead of stemming technique.\n","\n","**Note**<br>\n","In the following code , before stemming , we are deleting all stopwords from each sample and then afford its stemmig for each word.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:28.152572Z","iopub.status.busy":"2023-11-03T10:43:28.151872Z","iopub.status.idle":"2023-11-03T10:43:29.135678Z","shell.execute_reply":"2023-11-03T10:43:29.134778Z","shell.execute_reply.started":"2023-11-03T10:43:28.152542Z"},"trusted":true},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(test_text)\n","for token in doc :\n","    print(f'{token} => {token.lemma_}')"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:43:33.190796Z","iopub.status.busy":"2023-11-03T10:43:33.190461Z","iopub.status.idle":"2023-11-03T10:44:01.680602Z","shell.execute_reply":"2023-11-03T10:44:01.679796Z","shell.execute_reply.started":"2023-11-03T10:43:33.190771Z"},"trusted":true},"outputs":[],"source":["# lemmatizer = WordNetLemmatizer() \n","Stemmer = PorterStemmer()\n","def stopwords_cleaner(text):\n","#     word = [lemmatizer.lemmatize(letter) for letter in text if letter not in stopwords_list]\n","    word = [Stemmer.stem(letter) for letter in text if letter not in stopwords_list]\n","    peasting = ' '.join(word)\n","    return peasting\n","df['Text'] = df['Text'].apply(lambda x : stopwords_cleaner(x))\n","# stopwords_cleaner(Tokenizer.tokenize(df.Text[100]))"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:44:10.216453Z","iopub.status.busy":"2023-11-03T10:44:10.215821Z","iopub.status.idle":"2023-11-03T10:44:10.225877Z","shell.execute_reply":"2023-11-03T10:44:10.224923Z","shell.execute_reply.started":"2023-11-03T10:44:10.216423Z"},"trusted":true},"outputs":[],"source":["df['Text'][:10].to_frame()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**WordCloud**<br>\n","\n","word Cloud is just a data visualization technique used for representing text data in which the size of each word indicates its frequency or <br>  importance.Now , I will show the most important words(common ones) exclude stopwords for each Mode.<br>\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:44:13.675906Z","iopub.status.busy":"2023-11-03T10:44:13.675579Z","iopub.status.idle":"2023-11-03T10:44:17.050507Z","shell.execute_reply":"2023-11-03T10:44:17.049588Z","shell.execute_reply.started":"2023-11-03T10:44:13.675872Z"},"trusted":true},"outputs":[],"source":["%%time\n","positive_reviews = df[df['Mode'] == 'Positive']['Text']\n","pos = ' '.join(map(str, positive_reviews))\n","pos_wordcloud = WordCloud(width=1500, height=800,\n","                          background_color='black',\n","                         stopwords=stopwords_list,\n","                          min_font_size=15).generate(pos)\n","plt.figure(figsize=(10, 10))\n","plt.imshow(pos_wordcloud)\n","plt.title('Word Cloud for Positive Reviews')\n","plt.axis('off')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:44:25.526124Z","iopub.status.busy":"2023-11-03T10:44:25.525439Z","iopub.status.idle":"2023-11-03T10:44:29.44454Z","shell.execute_reply":"2023-11-03T10:44:29.44365Z","shell.execute_reply.started":"2023-11-03T10:44:25.526073Z"},"trusted":true},"outputs":[],"source":["%%time\n","positive_reviews = df[df['Mode'] == 'Negative']['Text']\n","neg = ' '.join(map(str, positive_reviews))\n","pos_wordcloud = WordCloud(width=1500, height=800,\n","                          background_color='black',\n","                         stopwords=stopwords_list,\n","                          min_font_size=15).generate(neg)\n","plt.figure(figsize=(10, 10))\n","plt.imshow(pos_wordcloud)\n","plt.title('Word Cloud for Negative Reviews')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:44:46.308968Z","iopub.status.busy":"2023-11-03T10:44:46.308579Z","iopub.status.idle":"2023-11-03T10:44:49.796485Z","shell.execute_reply":"2023-11-03T10:44:49.795611Z","shell.execute_reply.started":"2023-11-03T10:44:46.308936Z"},"trusted":true},"outputs":[],"source":["%%time\n","positive_reviews = df[df['Mode'] == 'Neutral']['Text']\n","Neutral = ' '.join(map(str, positive_reviews))\n","pos_wordcloud = WordCloud(width=1500, height=800,\n","                          background_color='black',\n","                         stopwords=stopwords_list,\n","                          min_font_size=15).generate(Neutral)\n","plt.figure(figsize=(10, 10))\n","plt.imshow(pos_wordcloud)\n","plt.title('Word Cloud for Neutral Reviews')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:44:53.876313Z","iopub.status.busy":"2023-11-03T10:44:53.875406Z","iopub.status.idle":"2023-11-03T10:44:56.901124Z","shell.execute_reply":"2023-11-03T10:44:56.900089Z","shell.execute_reply.started":"2023-11-03T10:44:53.876276Z"},"trusted":true},"outputs":[],"source":["%%time\n","positive_reviews = df[df['Mode'] == 'Irrelevant']['Text']\n","Irrelevant  = ' '.join(map(str, positive_reviews))\n","pos_wordcloud = WordCloud(width=1500, height=800,\n","                          background_color='black',\n","                         stopwords=stopwords_list,\n","                          min_font_size=15).generate(Irrelevant )\n","plt.figure(figsize=(10, 10))\n","plt.imshow(pos_wordcloud)\n","plt.title('Word Cloud for Irrelevant Reviews')\n","plt.axis('off')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","A number of fundamental statistics are shown below to know about corpus .such as , maximum , minimum length of texts , <br>\n","mean of length , std and the last calculation which is momentous is the number of words which we are required to have for each sample <br>\n","we should not allow all words of each sample be in the review).For example, someone has a review with 797 words. so it is computiationally expensive and furthermore , it dosent allow our model to learn perfectly.Instead of all words , we are going to use a formula to keep 95% of data , but <br>\n","with a samller amount of data for learning better and instead of those additional words we use a special character namely PAD becuase all sample <br>\n","should have the same length. here , Mew + 2 sigma is 165.7 which we are going to use 166 words of each sample.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:06.13225Z","iopub.status.busy":"2023-11-03T10:45:06.131309Z","iopub.status.idle":"2023-11-03T10:45:06.199897Z","shell.execute_reply":"2023-11-03T10:45:06.198995Z","shell.execute_reply.started":"2023-11-03T10:45:06.132215Z"},"trusted":true},"outputs":[],"source":["len_text = [len(tx) for tx in df['Text'].to_list()]\n","print(f'Max Length : {np.max(len_text)}')\n","print(f'Min Length : {np.min(len_text)}')\n","print(f'Mean Length : {round(np.mean(len_text),2)}')\n","print(f'Std Length : {round(np.std(len_text),2)}')\n","print(f'Mew + 2sigma : {round(np.mean(len_text)+ 2 *np.std(len_text),2)}')\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","\n","Now , after removing all stop words we want to see what words are most common for each mode in our corpus.Like wordcloud <br>\n","but in a barchart to have better insights. (either wordcloud or this way is enough for representing)\n"," \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:10.245378Z","iopub.status.busy":"2023-11-03T10:45:10.244737Z","iopub.status.idle":"2023-11-03T10:45:12.818214Z","shell.execute_reply":"2023-11-03T10:45:12.817313Z","shell.execute_reply.started":"2023-11-03T10:45:10.245344Z"},"trusted":true},"outputs":[],"source":["%%time\n","for inx , value in enumerate(sentiments):\n","    \n","    counter = Counter(make_corpus(value))\n","    most_common = counter.most_common()\n","\n","    x = []\n","    y = []\n","\n","    for word, count in most_common[:40]:\n","         if word not in stop:\n","            x.append(word)\n","            y.append(count)\n","         \n","    sns.barplot(x=y, y=x, orient='h')\n","    plt.title(f'{value} most used words')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","\n","we are going to create a column namely sentiments and instead of Positive , Negative we put 1 and 0 , instead of Neutral and <br>\n","Irrelevant we put 2. It means that underestanding Positive and Negative reviews is much more important for use rather that neutral and <br>\n","Irrelevant ones.(Just for decreasing the calsses and increasing the accuracy for finding Positive and Negative ones) however , we are able to<br>\n"," classify each one of them without mixing , but of curse the accuracy will decrease noticeably.<br>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:18.183967Z","iopub.status.busy":"2023-11-03T10:45:18.183278Z","iopub.status.idle":"2023-11-03T10:45:18.243737Z","shell.execute_reply":"2023-11-03T10:45:18.242794Z","shell.execute_reply.started":"2023-11-03T10:45:18.183931Z"},"trusted":true},"outputs":[],"source":["df['sentiments'] = df['Mode'].replace({'Positive' : 1 ,  'Negative' : 0 ,'Neutral':2 , 'Irrelevant' : 2 })"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:21.956431Z","iopub.status.busy":"2023-11-03T10:45:21.955595Z","iopub.status.idle":"2023-11-03T10:45:21.970222Z","shell.execute_reply":"2023-11-03T10:45:21.969024Z","shell.execute_reply.started":"2023-11-03T10:45:21.956397Z"},"trusted":true},"outputs":[],"source":["df.sample(10)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","Now , we are going to create a customer class to show  len and item.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:25.015886Z","iopub.status.busy":"2023-11-03T10:45:25.014944Z","iopub.status.idle":"2023-11-03T10:45:25.023571Z","shell.execute_reply":"2023-11-03T10:45:25.02238Z","shell.execute_reply.started":"2023-11-03T10:45:25.015838Z"},"trusted":true},"outputs":[],"source":["class Dataset:\n","    def __init__(self,text,sentiment):\n","        self.text = text\n","        self.sentiment = sentiment\n","        \n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self,item):\n","        text = self.text[item,:]\n","        target = self.sentiment[item]\n","        return {\n","            \"text\": torch.tensor(text,dtype = torch.long),\n","            \"target\": torch.tensor(target,dtype = torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","\n","We require helper functions to help us reading word vectors(we have a vord vector to give our model to understand the distant meaning of each words<br>\n","and by that we can improve our model accuracy. There are many word vectors whcih I will use glove.6B.300d 300 is the dimention.you can use less <br>\n","ones. More is better understanding but computationally higher.)and them another function to help us create embedding matrix for our corpus words.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:28.450458Z","iopub.status.busy":"2023-11-03T10:45:28.449814Z","iopub.status.idle":"2023-11-03T10:45:28.45594Z","shell.execute_reply":"2023-11-03T10:45:28.454946Z","shell.execute_reply.started":"2023-11-03T10:45:28.450427Z"},"trusted":true},"outputs":[],"source":["def load_vectors(fname):\n","    fin = open(fname , encoding=\"utf8\")\n","    data = {}\n","    for line in fin:\n","        tokens = line.split()\n","        data[tokens[0]] = np.array([float(value) for value in tokens[1:]])\n","        \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:31.153149Z","iopub.status.busy":"2023-11-03T10:45:31.152777Z","iopub.status.idle":"2023-11-03T10:45:31.15848Z","shell.execute_reply":"2023-11-03T10:45:31.157507Z","shell.execute_reply.started":"2023-11-03T10:45:31.153118Z"},"trusted":true},"outputs":[],"source":["def create_embedding_matrix(word_index,embedding_dict):\n","\n","    embedding_matrix = np.zeros((len(word_index)+1,300))\n","    for word, i in word_index.items():\n","        if word in embedding_dict:\n","            embedding_matrix[i] = embedding_dict[word]\n","            \n","    return embedding_matrix"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","Creating our Model with the help of sentimentBiLSTM from nn.Module.Described each part in the code\n","\n","</div>\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<a id=\"1\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> Model </b></div>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<center>\n","<img src=\"https://miro.medium.com/v2/resize:fit:764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=800 height=500 />\n","</center>"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["\n","Bidirectional LSTM (BiLSTM) is a recurrent neural network used primarily on natural language processing. Unlike standard LSTM, the input flows in both directions, and it‚Äôs capable of utilizing information from both sides. It‚Äôs also a powerful tool for modeling the sequential dependencies between words and phrases in both directions of the sequence.\n","\n","In summary, BiLSTM adds one more LSTM layer, which reverses the direction of information flow. Briefly, it means that the input sequence flows backward in the additional LSTM layer. Then we combine the outputs from both LSTM layers in several ways, such as average, sum, multiplication, or concatenation."]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:35.166181Z","iopub.status.busy":"2023-11-03T10:45:35.165385Z","iopub.status.idle":"2023-11-03T10:45:35.176155Z","shell.execute_reply":"2023-11-03T10:45:35.174995Z","shell.execute_reply.started":"2023-11-03T10:45:35.166146Z"},"trusted":true},"outputs":[],"source":["class sentimentBiLSTM(nn.Module):\n","#inherited from nn.Module\n","    \n","    def __init__(self, embedding_matrix, hidden_dim, output_size):\n","      \n","        #initializing the params by initialization method \n","        super(sentimentBiLSTM, self).__init__()\n","        self.embedding_matrix = embedding_matrix\n","        self.hidden_dim = hidden_dim\n","        num_words = self.embedding_matrix.shape[0]\n","        embed_dim = self.embedding_matrix.shape[1]\n","        # craetinh embedding layer\n","        self.embedding = nn.Embedding(num_embeddings=num_words,embedding_dim=embed_dim)\n","        \n","        ## initializes the weights of the embedding layer to the pretrained embeddings in \n","        ## embedding_matrix. It first converts embedding_matrix to a PyTorch tensor and \n","        ## wraps it in an nn.Parameter object, which makes it a learnable parameter of the model.\n","        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n","        self.embedding.weight.requires_grad = False\n","        self.lstm = nn.LSTM(embed_dim,hidden_dim,bidirectional=True,batch_first=True)\n","\n","        #it is multuplied by 2 becuase it is bi_directional if one-sided it didnt need.\n","        self.fc = nn.Linear(hidden_dim*2, output_size)\n","        \n","\n","    #we need a forward function to model calculate the cost and know how bad the params is .  \n","    # However , it can be written in a line of code but if we want to track it it is easier way.  \n","    def forward(self, x):\n","\n","       \n","        embeds = self.embedding(x)\n","        lstm_out,_ = self.lstm(embeds)\n","        lstm_out = lstm_out[:, -1]\n","        out = self.fc(lstm_out)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","spliting data to train and test => 80% for train and 20% for test\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:47.08553Z","iopub.status.busy":"2023-11-03T10:45:47.085155Z","iopub.status.idle":"2023-11-03T10:45:47.133106Z","shell.execute_reply":"2023-11-03T10:45:47.132339Z","shell.execute_reply.started":"2023-11-03T10:45:47.085501Z"},"trusted":true},"outputs":[],"source":["y = df.sentiments.values\n","train_df,test_df = train_test_split(df,test_size = 0.2, stratify = y)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","Max_length as mew + 2sigma = 167 <br>\n","Batch size is a number that detremined based on your system 16-32-64...<br>\n","Hidden_dimention for the model will be 64<br>\n","output is the number of classes which we have (len(classes))<br>\n","Also check if Cuda is available we put our system on GPU else CPU\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:49.537283Z","iopub.status.busy":"2023-11-03T10:45:49.536587Z","iopub.status.idle":"2023-11-03T10:45:49.543156Z","shell.execute_reply":"2023-11-03T10:45:49.542121Z","shell.execute_reply.started":"2023-11-03T10:45:49.537252Z"},"trusted":true},"outputs":[],"source":["MAX_LEN = 167\n","BATCH_SIZE = 32\n","hidden_dim = 64\n","output_size = 3\n","\n","\n","if torch.cuda.is_available():\n","    \n","       device = torch.device(\"cuda\")\n","   \n","else:\n","       device = torch.device(\"cpu\")\n","    \n","\n","print(f'Current device is {device}')\n"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","now , we need to convert each sample to a readable way and deleting all extra words more than 167 and put PAD character instead <br>\n","we use texts_to_sequences function to do that.and then using DataLoader to read data for both Train and Test.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:45:53.093661Z","iopub.status.busy":"2023-11-03T10:45:53.093007Z","iopub.status.idle":"2023-11-03T10:45:56.21421Z","shell.execute_reply":"2023-11-03T10:45:56.213397Z","shell.execute_reply.started":"2023-11-03T10:45:53.093631Z"},"trusted":true},"outputs":[],"source":["tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(df.Text.values.tolist())\n","\n","xtrain = tokenizer.texts_to_sequences(train_df.Text.values)\n","xtest = tokenizer.texts_to_sequences(test_df.Text.values)\n","xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain,maxlen = MAX_LEN)\n","xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest,maxlen = MAX_LEN)\n","train_dataset = Dataset(text=xtrain,sentiment=train_df.sentiments.values)\n","train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,drop_last=True)\n","valid_dataset = Dataset(text=xtest,sentiment=test_df.sentiments.values)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=BATCH_SIZE,drop_last=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:46:10.760953Z","iopub.status.busy":"2023-11-03T10:46:10.760599Z","iopub.status.idle":"2023-11-03T10:46:10.834092Z","shell.execute_reply":"2023-11-03T10:46:10.833237Z","shell.execute_reply.started":"2023-11-03T10:46:10.760923Z"},"trusted":true},"outputs":[],"source":["# check a batch of data \n","one_batch = next(iter(train_loader))\n","one_batch"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","As I said , we need a word vector (glove.6B.300d) which Im using and now load it and tokenize it \n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:46:14.017904Z","iopub.status.busy":"2023-11-03T10:46:14.017545Z","iopub.status.idle":"2023-11-03T10:47:28.451628Z","shell.execute_reply":"2023-11-03T10:47:28.45073Z","shell.execute_reply.started":"2023-11-03T10:46:14.017875Z"},"trusted":true},"outputs":[],"source":["embedding_dict = load_vectors('/kaggle/input/glove6b300dtxt/glove.6B.300d.txt')\n","embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","Now create a object of the model (embedding_matrix, hidden_dim=64, output_size=3) and put the model on device.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:47:42.558532Z","iopub.status.busy":"2023-11-03T10:47:42.558195Z","iopub.status.idle":"2023-11-03T10:47:49.793354Z","shell.execute_reply":"2023-11-03T10:47:49.792389Z","shell.execute_reply.started":"2023-11-03T10:47:42.558507Z"},"trusted":true},"outputs":[],"source":["model = sentimentBiLSTM(embedding_matrix ,  hidden_dim, output_size)\n","model = model.to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","manual_seed : for reproductivity<br>\n","Optimizer : can be either SGD and Adam or any others . But I prefer Adam as defualt. with lr = 0.001<br>\n","cost_function = CrossEntropyLoss() cause we have 3 classes => BCELoss() if it was just 2 <br>\n","schedul_learning: also we can use a schedul_learning to decrease the lr after a number of epochs but I tried and i didnt go well<br>\n","acc : acc function is used  to show the accuray<br>\n","The number of epochs will be = 9 \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:47:57.822059Z","iopub.status.busy":"2023-11-03T10:47:57.82136Z","iopub.status.idle":"2023-11-03T10:47:57.82926Z","shell.execute_reply":"2023-11-03T10:47:57.828464Z","shell.execute_reply.started":"2023-11-03T10:47:57.82203Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(42) \n","torch.cuda.manual_seed(42)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","criterion = nn.CrossEntropyLoss() \n","# schedul_learning = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer , milestones=[6] ,\n","#                                                         gamma=0.055)\n","\n","def acc(pred,label):\n","    pred = pred.argmax(1)\n","    return torch.sum(pred == label.squeeze()).item()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:48:02.796142Z","iopub.status.busy":"2023-11-03T10:48:02.795773Z","iopub.status.idle":"2023-11-03T10:49:28.84848Z","shell.execute_reply":"2023-11-03T10:49:28.847478Z","shell.execute_reply.started":"2023-11-03T10:48:02.796113Z"},"trusted":true},"outputs":[],"source":["clip = 5\n","epochs = 9\n","valid_loss_min = np.Inf\n","# train for some number of epochs\n","epoch_tr_loss,epoch_vl_loss = [],[]\n","epoch_tr_acc,epoch_vl_acc = [],[]\n","\n","for epoch in range(epochs):\n","    # for getting loss and accuracy for train\n","    train_losses = []\n","    train_acc = 0.0\n","\n","    #put model on train mode\n","    model.train()\n","    correct = 0\n","\n","    # initialize hidden state \n","    for data in train_loader:  \n","\n","        #get text and target \n","        inputs = data['text']\n","        labels = data['target']\n","\n","        #put them on GPU and right dtypes\n","        inputs = inputs.to(device,dtype=torch.long)\n","        labels = labels.to(device,dtype=torch.float)\n","\n","         #gradient becomes zero=> avoid accumulating \n","        model.zero_grad()\n","        output = model(inputs)\n","          # calculate the loss and perform backprop\n","        loss = criterion(output, labels.long())\n","        loss.backward()\n","        train_losses.append(loss.item())\n","        # accuracy\n","        accuracy = acc(output,labels)\n","        train_acc += accuracy\n","        #`clip_grad_norm` helps prevent the exploding gradient problem in LSTMs\n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        \n","    # for getting loss and accuracy for valiadtion\n","    val_losses = []\n","    val_acc = 0.0\n","\n","    #put model on evaluation mode\n","    model.eval()\n","    for data in valid_loader:\n","\n","        #get text and target \n","        inputs = data['text']\n","        labels = data['target']\n","\n","        #put them on GPU and right dtypes\n","        inputs = inputs.to(device,dtype=torch.long)\n","        labels = labels.to(device,dtype=torch.float)\n","\n","        #gradient becomes zero=> avoid accumulating \n","        model.zero_grad()\n","        output = model(inputs)\n","\n","        output = model(inputs)\n","        #Loss calculating \n","        val_loss = criterion(output, labels.long())\n","        #append Loss to the above list\n","        val_losses.append(val_loss.item())\n","\n","        # calculating accuracy \n","        accuracy = acc(output,labels)\n","        val_acc += accuracy\n","        epoch_train_loss = np.mean(train_losses)\n","\n","        #using schedule lr if you need\n","#         schedul_learning.step()\n","#         schedul_learning\n","\n","    #appending all accuracy and loss to the above lists and variables\n","    epoch_val_loss = np.mean(val_losses)\n","    epoch_train_acc = train_acc/len(train_loader.dataset)\n","    epoch_val_acc = val_acc/len(valid_loader.dataset)\n","    epoch_tr_loss.append(epoch_train_loss)\n","    epoch_vl_loss.append(epoch_val_loss)\n","    epoch_tr_acc.append(epoch_train_acc)\n","    epoch_vl_acc.append(epoch_val_acc)\n","    print(f'Epoch {epoch+1}') \n","    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n","    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n","    if epoch_val_loss <= valid_loss_min:\n","        #each time that model(params) get better you can save the model(you have to enter a path ou you pc and save with pt file)\n","        # torch.save(model.state_dict(), r'C:\\Users\\payama\\Desktop\\Projects kaggle\\NLP\\vectors features\\BidirectionalLSTM.pt')\n","#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n","        print(f'Validation loss decreased ({valid_loss_min} --> {epoch_val_loss})  Saving model ...')\n","        # save model if better result happends\n","        valid_loss_min = epoch_val_loss\n","    print(30 * '==' , '>')"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n","\n","**Descripton**<br>\n","    \n","The result after 10 epochs is :<br>\n","    \n","train_accuracy : 92.44121135998884 <br>\n","val_accuracy : 82.91117158607216<br>\n","    \n","train_loss : 0.18834395279212413<br>\n","val_loss : 0.5528476672784594<br>  \n","\n"," \n","Lets plot it for better understanding =>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:49:46.05202Z","iopub.status.busy":"2023-11-03T10:49:46.051349Z","iopub.status.idle":"2023-11-03T10:49:46.475309Z","shell.execute_reply":"2023-11-03T10:49:46.474378Z","shell.execute_reply.started":"2023-11-03T10:49:46.051986Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(7,5))\n","plt.plot(range(1,10),epoch_tr_acc , label='train accuracy')\n","plt.scatter(range(1,10),epoch_tr_acc)\n","plt.plot(range(1,10),epoch_vl_acc , label='val accuracy')\n","plt.scatter(range(1,10),epoch_vl_acc)\n","plt.title('Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-11-03T10:49:50.296186Z","iopub.status.busy":"2023-11-03T10:49:50.295814Z","iopub.status.idle":"2023-11-03T10:49:50.646944Z","shell.execute_reply":"2023-11-03T10:49:50.646032Z","shell.execute_reply.started":"2023-11-03T10:49:50.296156Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(7,5))\n","plt.plot(range(1,10),epoch_tr_loss , label='train loss')\n","plt.scatter(range(1,10),epoch_tr_loss )\n","plt.plot(range(1,10),epoch_vl_loss , label='val loss')\n","plt.scatter(range(1,10),epoch_vl_loss)\n","plt.title('Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["<a id=\"1\"></a>\n","# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> End </b></div>"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5504,"sourceId":8240,"sourceType":"datasetVersion"},{"datasetId":1520310,"sourceId":2510329,"sourceType":"datasetVersion"}],"dockerImageVersionId":30558,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
